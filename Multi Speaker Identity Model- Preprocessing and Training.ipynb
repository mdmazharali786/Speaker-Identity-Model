{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f583d78",
   "metadata": {},
   "source": [
    "# Multi Speaker Identity Model\n",
    "This sample project is based on audio dataset extracted from ipl pre, post and mid match discussion on cricbuzz set. Audio was downloaded from youtube link. In a single audio file there should be exactly 3 speaker to run this notebook successfully.\n",
    "\n",
    "#### Problem Statement-\n",
    "In a single audio file, identify all the speakers and returns all the list of speakers name, for unidentified speaker model should return as Unknown speaker.\n",
    "\n",
    "#### How to Preparedataset?\n",
    "Download your own audio file in any valid audio format. In each audio file there should be 3 speakers. Including all speakers from all audio file there should be 5 unique speakers.\n",
    "\n",
    "<pre><b>Note:</b> If you have good GPU power and you can run the notebook in one go without saving diarization and embeddding stage you can update code and save some steps of processing. See below hardware requirement for smooth running.\n",
    "\n",
    "GPU: 8 GB / 12 GB\n",
    "RAM: 16 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca31312",
   "metadata": {},
   "source": [
    "## Loading required library files\n",
    "Make sure you have installed all the library listed in requirement.txt file. Also check if your GPU is up for cuda availablity. This notebook require GPU so make sure you have configured everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab667411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.audio import Pipeline, Audio\n",
    "from pyannote.core import Segment\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import os\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import IPython.display as ipd\n",
    "from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788042e4",
   "metadata": {},
   "source": [
    "## Preprocessing on audio file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b076af",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = Audio()\n",
    "\n",
    "def get_trim_audio(filepath, trim=True):\n",
    "    '''This function returns audio file in wavefrom\n",
    "    \n",
    "    Parameters-\n",
    "    filepath: audio file location\n",
    "    trim: if set true returned audio will will be trimed up to 30 seconds from both end\n",
    "    \n",
    "    Return: audio waveform and sample rate\n",
    "    '''\n",
    "    if trim:\n",
    "        dur = librosa.get_duration(filename=filepath)\n",
    "        start = 30\n",
    "        end = dur-30\n",
    "        clip = Segment(start, end)\n",
    "        wv, sr = audio.crop(filepath, clip)\n",
    "    else:\n",
    "        wv, sr = audio(file)\n",
    "    return wv, sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328620c0",
   "metadata": {},
   "source": [
    "### Speaker diarization stage\n",
    "Splitting each audio file into segments belonging to different speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393ced39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to set root directory to your audio dataset directory\n",
    "root = '/content/ipl'\n",
    "\n",
    "# here is pipeline initialization\n",
    "# this pipeline is used to diarize each audio file\n",
    "# it uses a pyannote pretrained model - pyannote/speaker-diarization-3.1 to diarize the audio file\n",
    "# tagging of speaker to each segment of the audio is not very accurate, \n",
    "# make sure to validate speaker in later stage of the preprocessing\n",
    "if not pipeline:\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", \n",
    "                                    use_auth_token='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')\n",
    "prep_data = []\n",
    "audio_file = os.listdir(root)\n",
    "for file in tqdm.tqdm(audio_file):\n",
    "    data_dict = {}\n",
    "    file_loc = os.path.join(root, file)\n",
    "    wave, rate = get_trim_audio(file_loc)\n",
    "    dia = pipeline.to(torch.device(0))({\"waveform\": wave, \"sample_rate\": rate}, num_speakers=3)\n",
    "    data_dict['fname'] = file\n",
    "    data_dict['segment'] = []\n",
    "    duration = wave/rate\n",
    "    for speech_turn, track, speaker in dia.itertracks(yield_label=True):\n",
    "        temp_dict = {}\n",
    "        temp_dict['start'] = speech_turn.start\n",
    "        temp_dict['end'] = speech_turn.end\n",
    "        temp_dict['track'] = track\n",
    "        temp_dict['speaker'] = speaker\n",
    "        data_dict['segment'].append(temp_dict)\n",
    "    \n",
    "    prep_data.append(data_dict)\n",
    "    #saving data after diarization of each audio file to avoid any data loss in case of system failure\n",
    "    with open('/content/drive/MyDrive/dia_data.pkl', 'wb') as fh:\n",
    "        pkl.dump(prep_data, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cbdc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the preprocessed data in prevous stage if kernel restared\n",
    "with open('dia_data.pkl', 'rb') as f:\n",
    "    prep_data = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a33dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting preprocessed data into panda's dataframe to ease some processing \n",
    "dia_df = pd.DataFrame()\n",
    "for i in range(len(prep_data)):\n",
    "    temp_df = pd.DataFrame(prep_data[i]['segment'])\n",
    "    temp_df['fname'] = prep_data[i]['fname']\n",
    "    dia_df = pd.concat([dia_df, temp_df], axis=0)\n",
    "\n",
    "# calculating time difference in seconds in delta columns\n",
    "dia_df['delta'] = dia_df['end']-dia_df['start']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489bab7",
   "metadata": {},
   "source": [
    "### Embedding Stage - Vector embedding\n",
    "Each segment of each audio file is embedded to 192 dimension of vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2073e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a pretrained embedding model of speechbrain that converts each audio segment into 192 dimension vector\n",
    "embedding_model = PretrainedSpeakerEmbedding(\n",
    "    \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "    device=torch.device(\"cuda\"))\n",
    "\n",
    "def segment_audio(start, end, data, rate):\n",
    "    '''Segment audio wave from start to end position and returns embedded data\n",
    "    \n",
    "    Parameters-\n",
    "    start: starting postion of the audio segment(in second)\n",
    "    end: ending postion of the audio segment(in second)\n",
    "    data: audio data in waveform\n",
    "    rate: sample rate of the audio\n",
    "    \n",
    "    Return-\n",
    "    embedded data of segmented audio wave of shapes(1,1,192)\n",
    "    '''\n",
    "    start = round(start*rate)\n",
    "    end = round(end*rate)\n",
    "    data = data[:1,start:end]\n",
    "    return embedding_model(data[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d91772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting embedded data in below dictionary \n",
    "embedded_data_dict = {'fname':[], 'track':[], 'embedding': []}\n",
    "\n",
    "for i, f in tqdm(enumerate(up_list)):\n",
    "    filepath = os.path.join(root, f)\n",
    "    waveform, sample_rate = get_trim_audio(filepath):\n",
    "    # here filtering out all segments whose length is less than 1 seconds\n",
    "    # you can change as per requirement, 2 seconds is recommended if you have enough audio data\n",
    "    dia_segment_df = dia_df[(dia_df['fname']==f) & (dia_df['delta']>=1)]\n",
    "\n",
    "    for st,end,fn,tr in dia_segment_df[['start','end','fname','track']].values:\n",
    "        embedded_segment = segment_audio(st, end, waveform, sample_rate)\n",
    "        embedded_data_dict['fname'].append(fn)\n",
    "        embedded_data_dict['track'].append(tr)\n",
    "        embedded_data_dict['embedding'].append(embedded_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae16925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving embedded data to avoid any loss in case of system failure or kernel restart in later stage\n",
    "with open('/content/drive/MyDrive/embedding.pkl', 'wb') as emb_fh:\n",
    "      pkl.dump(embedded_data_dict, emb_fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file if kernel restarted\n",
    "with open('embedding.pkl', 'rb') as emb:\n",
    "    embedded_data = pkl.load(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted to dataframe\n",
    "labled_df = pd.DataFrame(embedded_data)\n",
    "\n",
    "#filter out segment of length less than 1 seconds in data we get from diarization stage\n",
    "dia_df_1_plus = dia_df[dia_df['delta']>=1]\n",
    "dia_df_1_plus = dia_df_1_plus.reset_index(drop=True)\n",
    "\n",
    "#merging data we get from diarization stage and embedding stage and saving it as csv\n",
    "merged_with_lable = pd.merge(dia_df_1_plus,labled_df, on=['track','fname'])\n",
    "merged_with_lable.to_csv('merged_with_lable.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faf0763",
   "metadata": {},
   "source": [
    "### Speaker label varifation and actual speaker assignment stage\n",
    "Since we know the pretrained model 'speaker-diarization-3.1' used in diarization stage returns as label SPEAKER_00, SPEAKER_01 and SPEAKER_02 for each audio file, so you need to map each of this label to actual speaker name. And also we know that this pretrained model is not 100% accurate. So you have to verify 3-5 samples from each audio file and change name of the speaker to correct belonging if found incorrect. More you verify the sample more the model will perform better. So the recommendation is if you found incorrect speaker assignment for any audio file please go through all the segment of that audio file and assign correct speaker name for each track or segment of the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3731e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose you want to check for this file and assign actual label for SPEAKER_00/01/02\n",
    "# run this cell and play the showing audio player and make note of the speaker \n",
    "# For example SPEAKER_00 belongs Kunal, SPEAKER 01 belongs to Shivam make the note and change it as given in next cell\n",
    "f = 'XZGPRVC7kH8 Harsha_Bhogle Simon_Doull Gaurav_Kapur.mp4'\n",
    "waveform, sample_rate = get_trim_audio(os.path.join(root, f)):\n",
    "temp_df = merged_with_lable[(merged_with_lable['fname']==f)]\n",
    "for st, en, sp,tr in temp_df[['start','end', 'speaker', 'track']].values:\n",
    "    # remember I have trimed the audio 30 seconds from both end so if you have not trimed your audio\n",
    "    # please change 30 to 0 in print statement to get exact frame of the speaker\n",
    "    print(\"start: %d:%02d end: %d:%02d %s %s\" % ((st+30)//60, (st+30)%60, (en+30)//60, (en+30)%60, sp, tr))\n",
    "    # this display a audio playing bar\n",
    "    ipd.display(ipd.Audio(data=segment_audio(st, en, waveform, sample_rate), rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbedcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we have talked about assigning exact speaker name to dummy speaker label\n",
    "# here is the statement that is used \n",
    "# run this everytime you change the dummy and exact label \n",
    "# in below statement we wanted to change SPEAKER_00 to Simon Doull in audio file 'f'\n",
    "merged_with_lable['speaker'][(merged_with_lable['fname']==f) & (merged_with_lable['speaker']=='SPEAKER_00')]='Simon_Doull'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac2d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to csv for the work done till here\n",
    "merged_with_lable = pd.read_csv('merged_with_lable.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc4c77",
   "metadata": {},
   "source": [
    "## Splitting train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc46f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_file, test_file = train_test_split(merged_with_lable, test_size=0.1, stratify=merged_with_lable.speaker, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "74b8e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this preprocessing requires when you reload saved csv it stores embedded data of 192 dimension tensor into string\n",
    "# so this funtion converts back to numpy array\n",
    "def preprocessing(data):\n",
    "    temp_data = []\n",
    "    for i in data:\n",
    "        x = np.array(list(map(float, i[2:-2].split())))\n",
    "        temp_data.append(x)\n",
    "    return np.array(temp_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5492b5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shapes:  (1980, 192) (1980,)\n",
      "Test Shapes:  (220, 192) (220,)\n"
     ]
    }
   ],
   "source": [
    "X_train = preprocessing(train_file['embedding'])\n",
    "y_train = train_file['speaker'].values\n",
    "\n",
    "X_test = preprocessing(test_file['embedding'])\n",
    "y_test = test_file['speaker'].values\n",
    "\n",
    "print('Train Shapes: ', X_train.shape, y_train.shape)\n",
    "print('Test Shapes: ', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b3a72319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2cc89d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving this label encoder for future use to convert back to actual label from numeric value\n",
    "# this file is also used in model api to convert back to actual label\n",
    "with open('label_enc.pkl', 'wb') as fh:\n",
    "    pkl.dump(le, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5476b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform( X_train )\n",
    "X_test = scaler.transform( X_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e445fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving this file for the standardization of query audio file given in model api\n",
    "with open('scaler.pkl', 'wb') as fh:\n",
    "    pkl.dump(scaler, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafa5641",
   "metadata": {},
   "source": [
    "## Model Creation - Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e67019f2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 2s 115ms/step - loss: 1.3573 - accuracy: 0.5682 - val_loss: 0.8965 - val_accuracy: 0.7551\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.7256 - accuracy: 0.7992 - val_loss: 0.8520 - val_accuracy: 0.8056\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.5261 - accuracy: 0.8194 - val_loss: 0.8461 - val_accuracy: 0.7702\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 1s 88ms/step - loss: 0.4423 - accuracy: 0.8479 - val_loss: 0.7372 - val_accuracy: 0.7828\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 1s 85ms/step - loss: 0.3990 - accuracy: 0.8624 - val_loss: 0.6477 - val_accuracy: 0.8056\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.3338 - accuracy: 0.8756 - val_loss: 0.6423 - val_accuracy: 0.7904\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 1s 87ms/step - loss: 0.3155 - accuracy: 0.8920 - val_loss: 0.5877 - val_accuracy: 0.8131\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 1s 103ms/step - loss: 0.2752 - accuracy: 0.9015 - val_loss: 0.5608 - val_accuracy: 0.8056\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 1s 86ms/step - loss: 0.2176 - accuracy: 0.9223 - val_loss: 0.5325 - val_accuracy: 0.8131\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 1s 84ms/step - loss: 0.2167 - accuracy: 0.9179 - val_loss: 0.5592 - val_accuracy: 0.7955\n",
      "Epoch 10: early stopping\n"
     ]
    }
   ],
   "source": [
    "#Creating a Model\n",
    "from keras import models\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "import keras\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# model \n",
    "input_layer = layers.Input(shape=(192,))\n",
    "h_layers = layers.Dense(2048, activation='relu')(input_layer)\n",
    "h_layers = layers.Dropout(0.3)(h_layers)\n",
    "h_layers = layers.Dense(1024, activation='relu')(h_layers)\n",
    "h_layers = layers.BatchNormalization()(h_layers)\n",
    "h_layers = layers.Dropout(0.5)(h_layers)\n",
    "h_layers = layers.Dense(512, activation='relu')(h_layers)\n",
    "h_layers = layers.BatchNormalization()(h_layers)\n",
    "h_layers = layers.Dropout(0.3)(h_layers)\n",
    "h_layers = layers.Dense(256, activation='relu')(h_layers)\n",
    "h_layers = layers.Dropout(0.3)(h_layers)\n",
    "output = layers.Dense(5, activation='softmax')(h_layers)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', verbose=0, mode='min')\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    validation_split=0.2,\n",
    "                    epochs=100,\n",
    "                    batch_size=256, \n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9c38441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 192)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              395264    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1024)              4096      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3156997 (12.04 MB)\n",
      "Trainable params: 3153925 (12.03 MB)\n",
      "Non-trainable params: 3072 (12.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a2b65fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5570 - accuracy: 0.8318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5570483207702637, 0.831818163394928]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model Evaluation\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7fbd6db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ML_AI\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# this saved model is used for prediction in flask api\n",
    "model.save('./model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1750c12",
   "metadata": {},
   "source": [
    "## Confusion Matrix- Visualize the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ea7183b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predicting for all test data that model has not seen\n",
    "y_test_pred = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    score = model.predict(X_test[i].reshape(-1,192), verbose=0)\n",
    "    y_test_pred.append(np.argmax(score))\n",
    "    #print('Speaker: ', le.inverse_transform([np.argmax(score)])[0], 'True Lable: ', le.inverse_transform([y_test[i]])[0], \"Probability: \", score[0,np.argmax(score)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8e070fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x1fd028100a0>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
